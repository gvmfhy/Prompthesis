#!/usr/bin/env python3
"""
Drug Target Discovery Pipeline for GSE46602 with Bioconductor-Based Probe Mapping

This pipeline processes gene expression data from a GEO Series Matrix file,
maps probes to gene symbols using R's Bioconductor packages via rpy2,
performs differential expression analysis, constructs a gene co-expression
network, identifies potential drug targets through network analysis,
validates targets using the Open Targets API, and generates visualizations
and a summary report.

### differential is not working right now, probe mapping is fixed
"""

import os
import sys
import argparse
import logging
import pandas as pd
import numpy as np
import gzip
import re
import requests
from sklearn.preprocessing import StandardScaler
from scipy import stats
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import subprocess
import tempfile
import csv
from statsmodels.stats.multitest import multipletests
import traceback

# rpy2 integration for Bioconductor-based probe mapping
try:
    import rpy2.robjects as ro
    from rpy2.robjects.packages import importr
    from rpy2.robjects import pandas2ri
    pandas2ri.activate()
except ImportError:
    raise ImportError("rpy2 is required for Bioconductor-based probe mapping. Please install it via pip.")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stderr)]
)
logger = logging.getLogger('drug_target_pipeline')

def map_probes_to_genes_bioc(probe_ids):
    """
    Map probe IDs to gene symbols using Bioconductor.
    This function uses a different approach by calling an R script directly
    instead of using rpy2 interfaces, which helps avoid conversion issues.
    
    Args:
        probe_ids (list): List of probe IDs (strings)
        
    Returns:
        dict: Mapping of probe IDs to gene symbols
    """
    # Create temporary directory for input/output files
    temp_dir = tempfile.mkdtemp()
    probes_file = os.path.join(temp_dir, "probe_ids.txt")
    output_file = os.path.join(temp_dir, "probe_mapping.csv")
    
    # Write probe IDs to temporary file
    logger.info(f"Writing {len(probe_ids)} probe IDs to temporary file")
    with open(probes_file, 'w') as f:
        for probe_id in probe_ids:
            f.write(f"{probe_id}\n")
    
    # Create R script for mapping
    r_script = os.path.join(temp_dir, "map_probes.R")
    r_code = """
    # Load required packages
    if (!requireNamespace("BiocManager", quietly = TRUE))
        install.packages("BiocManager", repos = "https://cloud.r-project.org")
    
    required_packages <- c("AnnotationDbi", "hgu133plus2.db", "GEOquery")
    for (pkg in required_packages) {
        if (!requireNamespace(pkg, quietly = TRUE))
            BiocManager::install(pkg, ask = FALSE)
    }
    
    library(AnnotationDbi)
    library(hgu133plus2.db)
    library(GEOquery)
    
    # Read probe IDs
    args <- commandArgs(trailingOnly = TRUE)
    probe_file <- args[1]
    output_file <- args[2]
    
    probes <- readLines(probe_file)
    cat("Read", length(probes), "probe IDs\\n")
    
    # Try direct mapping first
    mapping_df <- NULL
    
    # Method 1: Using select with direct package
    tryCatch({
        cat("Trying direct select method...\\n")
        valid_keys <- keys(hgu133plus2.db, keytype="PROBEID")
        valid_probes <- probes[probes %in% valid_keys]
        cat("Valid probes:", length(valid_probes), "out of", length(probes), "\\n")
        
        if (length(valid_probes) > 0) {
            result <- select(hgu133plus2.db,
                             keys = valid_probes,
                             columns = "SYMBOL",
                             keytype = "PROBEID")
            if (nrow(result) > 0) {
                mapping_df <- result
                cat("Direct mapping found", nrow(result), "mappings\\n")
            }
        }
    }, error = function(e) {
        cat("Error in direct mapping:", conditionMessage(e), "\\n")
    })
    
    # Method 2: Using GEOquery platform annotation
    if (is.null(mapping_df) || nrow(mapping_df) == 0) {
        tryCatch({
            cat("Trying GEOquery platform annotation...\\n")
            gpl <- getGEO("GPL570")
            anno_table <- Table(gpl)
            
            # Find ID and Symbol columns
            id_col <- NULL
            for (col_name in c("ID", "SPOT_ID", "PROBEID", "Probe Set ID")) {
                if (col_name %in% colnames(anno_table)) {
                    id_col <- col_name
                    break
                }
            }
            
            symbol_col <- NULL
            for (col_name in c("Gene Symbol", "SYMBOL", "gene_symbol", "Gene_Symbol")) {
                if (col_name %in% colnames(anno_table)) {
                    symbol_col <- col_name
                    break
                }
            }
            
            if (!is.null(id_col) && !is.null(symbol_col)) {
                cat("Using columns:", id_col, "->", symbol_col, "\\n")
                
                # Create mapping dataframe
                mapping_df <- data.frame(
                    PROBEID = anno_table[[id_col]],
                    SYMBOL = anno_table[[symbol_col]],
                    stringsAsFactors = FALSE
                )
                
                # Filter to only include requested probes
                mapping_df <- mapping_df[mapping_df$PROBEID %in% probes,]
                cat("Platform annotation found", nrow(mapping_df), "mappings\\n")
            } else {
                cat("Could not find appropriate ID/Symbol columns in platform annotation\\n")
            }
        }, error = function(e) {
            cat("Error in GEOquery mapping:", conditionMessage(e), "\\n")
        })
    }
    
    # Method 3: Using environment mapping
    if (is.null(mapping_df) || nrow(mapping_df) == 0) {
        tryCatch({
            cat("Trying environment mapping...\\n")
            symbol_list <- as.list(hgu133plus2SYMBOL)
            probe_ids <- names(symbol_list)
            symbols <- as.character(symbol_list)
            
            mapping_df <- data.frame(
                PROBEID = probe_ids,
                SYMBOL = symbols,
                stringsAsFactors = FALSE
            )
            
            # Filter to only include requested probes
            mapping_df <- mapping_df[mapping_df$PROBEID %in% probes,]
            cat("Environment mapping found", nrow(mapping_df), "mappings\\n")
        }, error = function(e) {
            cat("Error in environment mapping:", conditionMessage(e), "\\n")
        })
    }
    
    # Write output file
    if (!is.null(mapping_df) && nrow(mapping_df) > 0) {
        # Remove rows with empty symbols
        mapping_df <- mapping_df[mapping_df$SYMBOL != "" & !is.na(mapping_df$SYMBOL),]
        write.csv(mapping_df, output_file, row.names = FALSE)
        cat("Wrote", nrow(mapping_df), "mappings to output file\\n")
    } else {
        # Write empty file
        write.csv(data.frame(PROBEID=character(), SYMBOL=character()), output_file, row.names = FALSE)
        cat("No mappings found, wrote empty output file\\n")
    }
    """
    
    with open(r_script, 'w') as f:
        f.write(r_code)
    
    # Run R script
    logger.info("Running R script for probe mapping")
    cmd = ["Rscript", r_script, probes_file, output_file]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        logger.info(f"R script output: {result.stdout.strip()}")
    except subprocess.CalledProcessError as e:
        logger.error(f"R script failed with code {e.returncode}: {e.stderr.strip()}")
        logger.info(f"R script output: {e.stdout.strip()}")
        return {}
    
    # Read mapping results
    mapping = {}
    try:
        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
            with open(output_file, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    probe_id = row['PROBEID']
                    symbol = row['SYMBOL']
                    if symbol and symbol.strip():  # Check for non-empty symbol
                        mapping[probe_id] = symbol.strip()
            
            # Log mapping statistics
            total_probes = len(probe_ids)
            mapped_probes = len(mapping)
            mapping_percent = mapped_probes/total_probes*100 if total_probes > 0 else 0
            logger.info(f"Successfully mapped {mapped_probes} out of {total_probes} probes ({mapping_percent:.1f}%)")
        else:
            logger.error("Output file is empty or doesn't exist")
    except Exception as e:
        logger.error(f"Error reading mapping results: {str(e)}")
    
    # Clean up temporary files
    try:
        for file in [probes_file, output_file, r_script]:
            if os.path.exists(file):
                os.remove(file)
        os.rmdir(temp_dir)
    except Exception as e:
        logger.warning(f"Error cleaning up temporary files: {str(e)}")
    
    return mapping

class DrugTargetPipeline:
    def __init__(self, matrix_file, output_dir=None, p_threshold=0.05, fc_threshold=1.0):
        """
        Initialize the pipeline.
        
        Args:
            matrix_file (str): Path to GEO Series Matrix file.
            output_dir (str): Directory to save results.
            p_threshold (float): P-value threshold.
            fc_threshold (float): Log2 fold change threshold.
        """
        self.matrix_file = matrix_file
        self.output_dir = output_dir or os.path.join("results", f"GSE46602_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.p_threshold = p_threshold
        self.fc_threshold = fc_threshold
        
        os.makedirs(os.path.join(self.output_dir, "data"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "figures"), exist_ok=True)
        
        self.expression_data = None
        self.metadata = None
        self.normalized_data = None
        self.differential_results = None
        self.significant_genes = None
        self.network = None
        self.target_scores = None
        
        logger.info(f"Pipeline initialized with matrix file: {matrix_file}")
        logger.info(f"Results will be saved to: {self.output_dir}")
    
    def parse_geo_matrix(self):
        """Parse the GEO Series Matrix file to extract expression data and metadata."""
        logger.info(f"Parsing GEO Series Matrix file: {self.matrix_file}")
        metadata_lines = []
        data_lines = []
        sample_ids = []
        in_data_section = False
        
        try:
            # Check if the file is gzipped
            is_gzipped = self.matrix_file.lower().endswith('.gz')
            
            # Open the file with the appropriate method
            if is_gzipped:
                logger.info("Opening as gzipped file")
                file_opener = lambda: gzip.open(self.matrix_file, 'rt')
            else:
                logger.info("Opening as plain text file")
                file_opener = lambda: open(self.matrix_file, 'rt')
                
            with file_opener() as f:
                for line in f:
                    if line.startswith('!'):
                        metadata_lines.append(line.strip())
                    elif line.startswith('#'):
                        continue
                    elif "ID_REF" in line and not line.startswith('!'):
                        sample_ids = line.strip().split('\t')[1:]
                        in_data_section = True
                    elif in_data_section:
                        data_lines.append(line.strip())
        except Exception as e:
            logger.error(f"Error reading matrix file: {e}")
            raise
        
        self._extract_metadata(metadata_lines, sample_ids)
        self._extract_expression_data(data_lines, sample_ids)
        logger.info(f"Parsed {len(self.expression_data)} genes/probes and {len(self.metadata)} samples")
        return self.expression_data, self.metadata

    def _extract_metadata(self, metadata_lines, sample_ids):
        """Extract sample metadata from the matrix file metadata lines."""
        logger.info("Extracting metadata from GEO Series Matrix file")
        
        # Initialize metadata dictionary
        metadata_dict = {sample_id: {} for sample_id in sample_ids}
        
        # Track which metadata fields we find
        found_fields = set()
        
        # Process each metadata line
        for line in metadata_lines:
            if line.startswith('!Sample_'):
                try:
                    # Split on first equals sign
                    parts = line.split('=', 1)
                    if len(parts) != 2:
                        continue
                    
                    key = parts[0].replace('!Sample_', '').strip()
                    values = parts[1].strip().split('\t')
                    
                    # Handle the case where first value is empty (common in GEO files)
                    if len(values) > 1 and not values[0].strip():
                        values = values[1:]
                    
                    # Only process if we have the right number of values
                    if len(values) == len(sample_ids):
                        found_fields.add(key)
                        for i, sample_id in enumerate(sample_ids):
                            metadata_dict[sample_id][key] = values[i].strip('"')  # Remove quotes if present
                    else:
                        logger.warning(f"Metadata field '{key}' has {len(values)} values, expected {len(sample_ids)}")
                except Exception as e:
                    logger.warning(f"Error processing metadata line: {line[:50]}... - {str(e)}")
                    continue
        
        # Convert to DataFrame
        self.metadata = pd.DataFrame.from_dict(metadata_dict, orient='index')
        
        # Log found metadata fields
        logger.info(f"Found metadata fields: {', '.join(sorted(found_fields))}")
        
        # Try to identify sample groups
        group_fields = []
        for col in self.metadata.columns:
            unique_vals = self.metadata[col].unique()
            if 2 <= len(unique_vals) <= 5:  # Reasonable number for experimental groups
                group_fields.append((col, len(unique_vals), unique_vals))
        
        if group_fields:
            logger.info("Potential group fields found:")
            for field, n_vals, vals in group_fields:
                logger.info(f"  {field}: {n_vals} values - {', '.join(map(str, vals))}")
        else:
            logger.warning("No potential group fields found in metadata")
        
        # Save metadata to file
        metadata_path = os.path.join(self.output_dir, "data", "GSE46602_metadata.csv")
        self.metadata.to_csv(metadata_path)
        logger.info(f"Metadata saved to {metadata_path}")
        
        # Add a default group if none exists
        if len(self.metadata.columns) == 0 or not any(col.lower() in ['group', 'condition', 'status', 'disease'] for col in self.metadata.columns):
            logger.warning("No group information found in metadata, creating default groups")
            # Assuming first half are controls, second half are cases (common in GEO data)
            n_samples = len(sample_ids)
            self.metadata['condition'] = ['control' if i < n_samples//2 else 'case' for i in range(n_samples)]
            logger.info(f"Created default groups: {n_samples//2} controls, {n_samples - n_samples//2} cases")
        
        return self.metadata

    def _extract_expression_data(self, data_lines, sample_ids):
        """Extract expression data from the matrix file."""
        data_rows = []
        row_ids = []
        
        for line in data_lines:
            parts = line.split('\t')
            if len(parts) >= len(sample_ids) + 1:
                # Clean up probe ID - remove quotes, colons, and other special characters
                probe_id_raw = parts[0]
                
                # Handle different formats
                # Format 1: 54679:"AFFX-BioB-3_at" -> AFFX-BioB-3_at
                # Format 2: 54675-"91816_f_at" -> 91816_f_at
                
                # First, extract the actual probe ID from any numbering or prefix
                if ':' in probe_id_raw:
                    probe_id_raw = probe_id_raw.split(':', 1)[1]
                elif '-' in probe_id_raw:
                    probe_id_raw = probe_id_raw.split('-', 1)[1]
                    
                # Then, remove quotes
                probe_id = probe_id_raw.strip('"\'')
                
                # Clean up any other potential issues
                probe_id = probe_id.strip()
                
                row_ids.append(probe_id)
                
                # Extract expression values
                try:
                    values = [float(v) for v in parts[1:len(sample_ids)+1]]
                    data_rows.append(values)
                except ValueError:
                    logger.warning(f"Could not convert expression values for row {probe_id}")
        
        if data_rows:
            self.expression_data = pd.DataFrame(data_rows, index=row_ids, columns=sample_ids)
        else:
            logger.error("No expression data could be parsed.")
            self.expression_data = pd.DataFrame()
    
    def preprocess_data(self):
        """Preprocess the expression data (NA handling, log2 transformation, normalization)."""
        logger.info("Preprocessing expression data")
        if self.expression_data is None:
            logger.error("No expression data available. Run parse_geo_matrix() first.")
            return None
        logger.info(f"Original data shape: {self.expression_data.shape}")
        logger.info(f"Missing values: {self.expression_data.isna().sum().sum()}")
        threshold = 0.2 * self.expression_data.shape[1]
        filtered_data = self.expression_data.dropna(thresh=threshold)
        logger.info(f"After dropping rows with >20% NaNs: {filtered_data.shape}")
        filled_data = filtered_data.apply(lambda row: row.fillna(row.median()), axis=1)
        data_max = filled_data.max().max()
        if data_max > 100:
            logger.info("Applying log2 transformation")
            filled_data = np.log2(filled_data + 1)
        scaler = StandardScaler()
        normalized_values = scaler.fit_transform(filled_data.T).T
        self.normalized_data = pd.DataFrame(normalized_values, index=filled_data.index, columns=filled_data.columns)
        norm_path = os.path.join(self.output_dir, "data", "GSE46602_normalized.csv")
        self.normalized_data.to_csv(norm_path)
        logger.info(f"Normalized data saved to {norm_path}")
        return self.normalized_data
    
    def map_probes_to_genes(self):
        """Map probe IDs to gene symbols using Bioconductor via rpy2."""
        logger.info("Mapping probe IDs to gene symbols using Bioconductor")
        if self.normalized_data is None:
            logger.error("No normalized data available. Run preprocess_data() first.")
            return None
        
        # Get probe IDs
        probe_ids = list(self.normalized_data.index)
        
        # Map probes to genes
        probe_to_gene = map_probes_to_genes_bioc(probe_ids)
        
        # Create a new DataFrame with gene symbols
        gene_expr = self.normalized_data.copy()
        gene_expr['gene_symbol'] = [probe_to_gene.get(probe, f"UNKNOWN_{probe}") for probe in probe_ids]
        gene_expr = gene_expr.reset_index().rename(columns={'index': 'probe_id'})
        
        # Filter out unmapped probes (those starting with UNKNOWN_)
        mapped_expr = gene_expr[~gene_expr['gene_symbol'].str.startswith('UNKNOWN_')]
        logger.info(f"Filtered out {len(gene_expr) - len(mapped_expr)} unmapped probes")
        
        # Group by gene symbol and aggregate using median
        gene_aggregated = mapped_expr.groupby('gene_symbol').agg({
            col: 'median' for col in mapped_expr.columns if col not in ['probe_id', 'gene_symbol']
        })
        
        # Update normalized data with gene-mapped data
        self.normalized_data = gene_aggregated
        
        # Save gene-mapped data
        gene_path = os.path.join(self.output_dir, "data", "GSE46602_gene_mapped.csv")
        self.normalized_data.to_csv(gene_path)
        logger.info(f"Gene-mapped expression data saved to {gene_path}")
        
        return self.normalized_data
    
    def perform_differential_analysis(self):
        """Perform differential expression analysis between case and control groups."""
        logger.info("Performing differential expression analysis")
        
        # Use the condition column to divide samples into groups
        control_group = 'control'
        case_group = 'case'
        logger.info("Using column 'condition' to divide samples into groups")
        logger.info(f"Control group: {control_group}, Case group: {case_group}")
        
        # Get the sample IDs for each group based on the metadata
        control_samples = [sample for sample, condition in self.sample_groups.items() if condition == control_group]
        case_samples = [sample for sample, condition in self.sample_groups.items() if condition == case_group]
        
        logger.info(f"Control samples: {len(control_samples)}")
        logger.info(f"Case samples: {len(case_samples)}")
        
        # Check if sample IDs are in the normalized data
        missing_control_samples = [s for s in control_samples if s not in self.normalized_data.columns]
        missing_case_samples = [s for s in case_samples if s not in self.normalized_data.columns]
        
        if missing_control_samples:
            logger.warning(f"Some control samples are missing from the data: {missing_control_samples}")
            control_samples = [s for s in control_samples if s in self.normalized_data.columns]
        
        if missing_case_samples:
            logger.warning(f"Some case samples are missing from the data: {missing_case_samples}")
            case_samples = [s for s in case_samples if s in self.normalized_data.columns]
        
        if not control_samples or not case_samples:
            logger.error("Not enough samples for differential analysis")
            self.differential_results = pd.DataFrame(columns=['gene', 'log2FC', 'pvalue', 'adj_pvalue'])
            return
        
        # Create result dataframe
        results = []
        
        try:
            # Perform t-test for each gene
            for gene in self.normalized_data.index:
                try:
                    # Get expression values for the gene in each group
                    # Make sure we're using the correct sample IDs that match the column names
                    control_expr = self.normalized_data.loc[gene, control_samples]
                    case_expr = self.normalized_data.loc[gene, case_samples]
                    
                    # Perform t-test
                    t_stat, p_value = stats.ttest_ind(case_expr, control_expr, equal_var=False)
                    
                    # Calculate log2 fold change
                    mean_case = np.mean(case_expr)
                    mean_control = np.mean(control_expr)
                    
                    # Avoid division by zero
                    if mean_control == 0:
                        log2fc = float('inf') if mean_case > 0 else float('-inf')
                    else:
                        log2fc = np.log2(mean_case / mean_control)
                    
                    results.append({
                        'gene': gene,
                        'log2FC': log2fc,
                        'pvalue': p_value
                    })
                except Exception as e:
                    logger.warning(f"Error analyzing gene {gene}: {str(e)}")
            
            # Convert to dataframe
            if results:
                diff_results = pd.DataFrame(results)
                
                # Calculate adjusted p-values (FDR)
                if 'pvalue' in diff_results.columns and not diff_results['pvalue'].isna().all():
                    diff_results['adj_pvalue'] = multipletests(diff_results['pvalue'].fillna(1), method='fdr_bh')[1]
                else:
                    diff_results['adj_pvalue'] = np.nan
                    
                # Sort by adjusted p-value
                diff_results = diff_results.sort_values('adj_pvalue')
                
                # Save results
                diff_path = os.path.join(self.output_dir, "data", "differential_results.csv")
                diff_results.to_csv(diff_path, index=False)
                logger.info(f"Differential analysis results saved to {diff_path}")
                
                self.differential_results = diff_results
            else:
                logger.warning("No differential expression results could be calculated")
                self.differential_results = pd.DataFrame(columns=['gene', 'log2FC', 'pvalue', 'adj_pvalue'])
                
        except Exception as e:
            logger.error(f"Error during differential analysis: {str(e)}")
            logger.error(traceback.format_exc())
            self.differential_results = pd.DataFrame(columns=['gene', 'log2FC', 'pvalue', 'adj_pvalue'])
    
    def construct_network(self, n_top_genes=500):
        """Construct a gene co-expression network based on Pearson correlations."""
        logger.info("Constructing gene co-expression network")
        if self.normalized_data is None:
            logger.error("Normalized expression data not available.")
            return None
        
        # Make sure we have enough genes for a meaningful network
        if len(self.normalized_data) < 2:
            logger.error(f"Not enough genes available in the normalized data. Found {len(self.normalized_data)}, need at least 2.")
            # Create empty network to avoid downstream errors
            network = nx.Graph()
            self.network = network
            logger.warning("Created empty network to avoid pipeline failure")
            return self.network
        
        if self.significant_genes is not None and len(self.significant_genes) > 0:
            top_genes = list(self.significant_genes['gene'])[:n_top_genes]
            logger.info(f"Using {len(top_genes)} significant genes for network construction")
        else:
            logger.warning("No significant genes found, using most variable genes instead")
            gene_var = self.normalized_data.var(axis=1).sort_values(ascending=False)
            top_genes = list(gene_var.index)[:n_top_genes]
            logger.info(f"Using {len(top_genes)} most variable genes for network construction")
        
        # Verify top_genes are actually in the normalized data
        valid_genes = [gene for gene in top_genes if gene in self.normalized_data.index]
        if len(valid_genes) < len(top_genes):
            logger.warning(f"Only {len(valid_genes)} out of {len(top_genes)} selected genes are in the normalized data")
            top_genes = valid_genes
        
        if len(top_genes) < 2:
            logger.error("Not enough valid genes for network construction")
            # Create empty network to avoid downstream errors
            network = nx.Graph()
            self.network = network
            logger.warning("Created empty network to avoid pipeline failure")
            return self.network
        
        expr_data = self.normalized_data.loc[top_genes]
        corr_matrix = expr_data.T.corr(method='pearson')
        corr_path = os.path.join(self.output_dir, "data", "correlation_matrix.csv")
        corr_matrix.to_csv(corr_path)
        logger.info(f"Correlation matrix saved to {corr_path}")
        
        threshold = 0.7
        network = nx.Graph()
        for gene in top_genes:
            network.add_node(gene)
        for i, gene1 in enumerate(top_genes):
            for gene2 in top_genes[i+1:]:
                corr = abs(corr_matrix.loc[gene1, gene2])
                if corr > threshold:
                    network.add_edge(gene1, gene2, weight=corr)
        self.network = network
        logger.info(f"Network constructed with {network.number_of_nodes()} nodes and {network.number_of_edges()} edges")
        nx.write_gexf(network, os.path.join(self.output_dir, "data", "gene_network.gexf"))
        return self.network
    
    def analyze_network(self):
        """Analyze the network to compute centrality measures and create a composite score."""
        logger.info("Analyzing network for potential drug targets")
        if self.network is None:
            logger.error("Network not available. Run construct_network() first.")
            return None
        
        # Handle empty or very small networks gracefully
        if self.network.number_of_nodes() < 2:
            logger.warning("Network has less than 2 nodes, cannot compute meaningful centrality measures")
            # Create a minimal result DataFrame to avoid pipeline failure
            centrality_df = pd.DataFrame({
                'gene': list(self.network.nodes()) if self.network.number_of_nodes() > 0 else ['PLACEHOLDER'],
                'degree_centrality': [0.0] * max(1, self.network.number_of_nodes()),
                'betweenness_centrality': [0.0] * max(1, self.network.number_of_nodes()),
                'eigenvector_centrality': [0.0] * max(1, self.network.number_of_nodes()),
                'composite_score': [0.0] * max(1, self.network.number_of_nodes())
            })
            centrality_path = os.path.join(self.output_dir, "data", "network_targets.csv")
            centrality_df.to_csv(centrality_path, index=False)
            logger.info(f"Minimal network analysis results saved to {centrality_path}")
            self.target_scores = centrality_df
            return self.target_scores
        
        try:
            degree_centrality = nx.degree_centrality(self.network)
            betweenness_centrality = nx.betweenness_centrality(self.network)
            eigenvector_centrality = nx.eigenvector_centrality(self.network, max_iter=1000)
            
            centrality_df = pd.DataFrame({
                'gene': list(self.network.nodes()),
                'degree_centrality': [degree_centrality[g] for g in self.network.nodes()],
                'betweenness_centrality': [betweenness_centrality[g] for g in self.network.nodes()],
                'eigenvector_centrality': [eigenvector_centrality[g] for g in self.network.nodes()]
            })
            
            # Normalize centrality metrics
            from sklearn.preprocessing import MinMaxScaler
            scaler = MinMaxScaler()
            centrality_df[['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']] = scaler.fit_transform(
                centrality_df[['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']]
            )
            
            centrality_df['composite_score'] = (
                centrality_df['degree_centrality'] +
                centrality_df['betweenness_centrality'] +
                centrality_df['eigenvector_centrality']
            ) / 3
            
            centrality_df = centrality_df.sort_values('composite_score', ascending=False)
            centrality_path = os.path.join(self.output_dir, "data", "network_targets.csv")
            centrality_df.to_csv(centrality_path, index=False)
            logger.info(f"Network analysis results saved to {centrality_path}")
            self.target_scores = centrality_df
            return self.target_scores
        
        except Exception as e:
            logger.error(f"Error during network analysis: {str(e)}")
            # Create a minimal result DataFrame to avoid pipeline failure
            centrality_df = pd.DataFrame({
                'gene': list(self.network.nodes()),
                'degree_centrality': [0.0] * self.network.number_of_nodes(),
                'betweenness_centrality': [0.0] * self.network.number_of_nodes(),
                'eigenvector_centrality': [0.0] * self.network.number_of_nodes(),
                'composite_score': [0.0] * self.network.number_of_nodes()
            })
            centrality_path = os.path.join(self.output_dir, "data", "network_targets.csv")
            centrality_df.to_csv(centrality_path, index=False)
            logger.warning(f"Created minimal centrality measures due to error: {str(e)}")
            self.target_scores = centrality_df
            return self.target_scores
    
    def _validate_gene_symbol(self, gene_symbol):
        """
        Validate if a string looks like a proper gene symbol.
        
        Args:
            gene_symbol (str): Gene symbol to validate
            
        Returns:
            bool: True if the symbol looks valid, False otherwise
        """
        if not isinstance(gene_symbol, str):
            return False
        
        # Most gene symbols are 1-20 characters long
        if len(gene_symbol) < 1 or len(gene_symbol) > 20:
            return False
        
        # Should not be a probe ID (e.g., '1234_at')
        if '_at' in gene_symbol.lower():
            return False
        
        # Should not be 'UNKNOWN_' prefixed
        if gene_symbol.startswith('UNKNOWN_'):
            return False
        
        # Should contain at least one letter
        if not any(c.isalpha() for c in gene_symbol):
            return False
        
        # Should not contain special characters except hyphen and dot
        if any(c not in '-.' and not c.isalnum() for c in gene_symbol):
            return False
        
        return True

    def _query_opentargets(self, gene_symbol):
        """Query the Open Targets GraphQL API for a given gene symbol."""
        if not self._validate_gene_symbol(gene_symbol):
            logger.warning(f"Skipping invalid gene symbol: {gene_symbol}")
            return None
        
        url = "https://api.platform.opentargets.org/api/v4/graphql"
        graphql_query = """
        query ($symbol: String!) {
          target(approvedSymbol: $symbol) {
            id
            approvedSymbol
            knownDrugs {
              drug {
                name
                maxPhase
              }
            }
            associations {
              score
              disease {
                name
                id
              }
            }
            tractability {
              smallMolecule {
                modality
                component
              }
              antibody {
                modality
                component
              }
            }
            safety {
              safetyLiabilities {
                event
                datasource
              }
            }
          }
        }
        """
        variables = {"symbol": gene_symbol}
        try:
            response = requests.post(url, json={"query": graphql_query, "variables": variables})
            response.raise_for_status()
            json_data = response.json()
            if "errors" in json_data:
                logger.error(f"Open Targets GraphQL errors for {gene_symbol}: {json_data['errors']}")
                return None
            return json_data.get("data", {})
        except requests.exceptions.RequestException as e:
            logger.error(f"Request failed for gene {gene_symbol}: {e}")
            return None

    def validate_targets(self, top_n=20):
        """Validate top targets using data from the Open Targets API."""
        logger.info("Validating top targets with Open Targets")
        if self.target_scores is None:
            logger.error("Target scores not available. Run analyze_network() first.")
            return None
        
        # Filter out invalid gene symbols first
        valid_targets = self.target_scores[
            self.target_scores['gene'].apply(self._validate_gene_symbol)
        ]
        
        if len(valid_targets) == 0:
            logger.error("No valid gene symbols found in target scores")
            return None
        
        logger.info(f"Found {len(valid_targets)} valid gene symbols out of {len(self.target_scores)} total targets")
        
        # Take top N valid targets
        top_targets = valid_targets.head(top_n)
        validation_results = []
        
        for _, target_row in top_targets.iterrows():
            gene_symbol = target_row['gene']
            result = self._query_opentargets(gene_symbol)
            
            if result and result.get("target"):
                target_data = result["target"]
                known_drugs = target_data.get("knownDrugs", [])
                num_known_drugs = len(known_drugs)
                max_phase = max([d.get('drug', {}).get('maxPhase', 0) for d in known_drugs], default=0)
                
                associations = target_data.get("associations", [])
                avg_association_score = np.mean([assoc.get("score", 0) for assoc in associations]) if associations else 0
                
                tractability = target_data.get("tractability", {})
                small_mol_tract = tractability.get("smallMolecule", [])
                antibody_tract = tractability.get("antibody", [])
                
                safety_data = target_data.get("safety", {})
                liabilities = safety_data.get("safetyLiabilities", [])
                
                # Calculate a composite drugability score
                drugability_score = (
                    (num_known_drugs * 0.3) +  # Known drugs weight
                    (max_phase * 0.2) +        # Clinical phase weight
                    (avg_association_score * 0.2) +  # Disease association weight
                    (len(small_mol_tract) * 0.15) +  # Small molecule tractability
                    (len(antibody_tract) * 0.15)     # Antibody tractability
                )
                
                validation_results.append({
                    "gene": gene_symbol,
                    "composite_score": target_row['composite_score'],
                    "num_known_drugs": num_known_drugs,
                    "max_clinical_phase": max_phase,
                    "avg_association_score": round(avg_association_score, 3),
                    "small_molecule_tractability": len(small_mol_tract),
                    "antibody_tractability": len(antibody_tract),
                    "num_safety_liabilities": len(liabilities),
                    "drugability_score": round(drugability_score, 3)
                })
                logger.info(f"Successfully validated {gene_symbol} (drugability score: {drugability_score:.3f})")
            else:
                logger.warning(f"No Open Targets data found for: {gene_symbol}")
                validation_results.append({
                    "gene": gene_symbol,
                    "composite_score": target_row['composite_score'],
                    "num_known_drugs": 0,
                    "max_clinical_phase": 0,
                    "avg_association_score": 0.0,
                    "small_molecule_tractability": 0,
                    "antibody_tractability": 0,
                    "num_safety_liabilities": 0,
                    "drugability_score": 0.0
                })
        
        final_targets = pd.DataFrame(validation_results)
        
        # Sort by drugability score
        final_targets = final_targets.sort_values('drugability_score', ascending=False)
        
        # Save results
        final_path = os.path.join(self.output_dir, "data", "GSE46602_final_targets.csv")
        final_targets.to_csv(final_path, index=False)
        logger.info(f"Validated targets with Open Targets saved to {final_path}")
        
        return final_targets
    
    def generate_visualizations(self):
        """Generate figures: volcano plot, network visualization, and bar plot of top targets."""
        logger.info("Generating visualizations")
        plt.style.use('seaborn-v0_8')
        
        # Volcano plot for differential expression
        if self.differential_results is not None and 'log2FC' in self.differential_results.columns and 'adj_pvalue' in self.differential_results.columns:
            plt.figure(figsize=(10, 8))
            plt.scatter(
                self.differential_results['log2FC'],
                -np.log10(self.differential_results['adj_pvalue']),
                alpha=0.5,
                color='gray',
                label='Not significant'
            )
            if self.significant_genes is not None and len(self.significant_genes) > 0:
                plt.scatter(
                    self.significant_genes['log2FC'],
                    -np.log10(self.significant_genes['adj_pvalue']),
                    alpha=0.8,
                    color='red',
                    label='Significant'
                )
            plt.axhline(-np.log10(self.p_threshold), linestyle='--', color='blue')
            plt.axvline(self.fc_threshold, linestyle='--', color='blue')
            plt.axvline(-self.fc_threshold, linestyle='--', color='blue')
            plt.xlabel('Log2 Fold Change')
            plt.ylabel('-Log10 Adjusted P-value')
            plt.title('Volcano Plot: Differential Expression')
            plt.legend()
            plt.tight_layout()
            plt.savefig(os.path.join(self.output_dir, "figures", "volcano_plot.png"), dpi=300)
            plt.close()
        else:
            logger.warning("Skipping volcano plot: differential_results not available or missing required columns")
        
        # Network visualization (subgraph for top 30 targets)
        if self.network is not None and self.target_scores is not None and self.network.number_of_nodes() > 0:
            top_genes = list(self.target_scores.head(min(30, len(self.target_scores)))['gene'])
            # Filter out placeholder genes
            top_genes = [gene for gene in top_genes if gene != 'PLACEHOLDER']
            
            if len(top_genes) > 1:  # Need at least 2 nodes for a meaningful network visualization
                subgraph = self.network.subgraph(top_genes)
                plt.figure(figsize=(12, 10))
                node_sizes = []
                for gene in subgraph.nodes():
                    score = self.target_scores.loc[self.target_scores['gene'] == gene, 'composite_score'].values
                    node_sizes.append(float(score[0]) * 1000 if len(score) > 0 else 500)
                
                edge_weights = [subgraph[u][v].get('weight', 0.5) * 2 for u, v in subgraph.edges()]
                pos = nx.spring_layout(subgraph, seed=42)
                nx.draw_networkx(
                    subgraph,
                    pos=pos,
                    with_labels=True,
                    node_size=node_sizes,
                    node_color='skyblue',
                    edge_color='gray',
                    width=edge_weights,
                    alpha=0.8,
                    font_size=10
                )
                plt.title('Top Genes Network')
                plt.axis('off')
                plt.tight_layout()
                plt.savefig(os.path.join(self.output_dir, "figures", "network_visualization.png"), dpi=300)
                plt.close()
            else:
                logger.warning("Skipping network visualization: Not enough nodes for meaningful visualization")
        else:
            logger.warning("Skipping network visualization: network or target_scores not available")
        
        # Bar plot for top potential targets
        if self.target_scores is not None and len(self.target_scores) > 0 and not all(gene == 'PLACEHOLDER' for gene in self.target_scores['gene']):
            top_n = min(20, len(self.target_scores))
            top_targets = self.target_scores.head(top_n)
            plt.figure(figsize=(12, 8))
            sns.barplot(
                x='composite_score',
                y='gene',
                data=top_targets,
                palette='viridis'
            )
            plt.title(f'Top {top_n} Potential Drug Targets')
            plt.xlabel('Composite Network Score')
            plt.ylabel('Gene')
            plt.tight_layout()
            plt.savefig(os.path.join(self.output_dir, "figures", "top_targets.png"), dpi=300)
            plt.close()
        else:
            logger.warning("Skipping bar plot: No valid target scores available")
        
        logger.info("Visualizations generated and saved to figures directory")
    
    def _create_summary_report(self):
        """Create a summary report of the pipeline results."""
        logger.info("Creating summary report")
        summary_path = os.path.join(self.output_dir, "summary.txt")
        with open(summary_path, 'w') as f:
            f.write("# Drug Target Discovery Pipeline Summary\n\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("Dataset: GSE46602\n\n")
            f.write("## Dataset Statistics\n")
            if self.expression_data is not None:
                f.write(f"- Samples: {self.expression_data.shape[1]}\n")
                f.write(f"- Probes/Genes: {self.expression_data.shape[0]}\n\n")
            f.write("## Differential Expression Analysis\n")
            if self.significant_genes is not None:
                f.write(f"- Significant genes: {len(self.significant_genes)}\n")
                up_genes = sum(self.significant_genes['log2FC'] > 0)
                down_genes = sum(self.significant_genes['log2FC'] < 0)
                f.write(f"- Up-regulated: {up_genes}\n")
                f.write(f"- Down-regulated: {down_genes}\n\n")
            f.write("## Network Analysis\n")
            if self.network is not None:
                f.write(f"- Network nodes: {self.network.number_of_nodes()}\n")
                f.write(f"- Network edges: {self.network.number_of_edges()}\n\n")
            f.write("## Top Potential Drug Targets\n")
            if self.target_scores is not None:
                top_10 = self.target_scores.head(10)
                for i, (_, target) in enumerate(top_10.iterrows()):
                    f.write(f"{i+1}. {target['gene']} (score: {target['composite_score']:.4f})\n")
        logger.info(f"Summary report saved to {summary_path}")
    
    def run_pipeline(self):
        """Run the complete drug target discovery pipeline."""
        logger.info("Starting drug target discovery pipeline")
        success = True
        try:
            self.parse_geo_matrix()
        except Exception as e:
            logger.error(f"Error during parsing GEO matrix: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            return False
            
        try:
            self.preprocess_data()
        except Exception as e:
            logger.error(f"Error during data preprocessing: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self.map_probes_to_genes()
        except Exception as e:
            logger.error(f"Error during probe-to-gene mapping: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self.perform_differential_analysis()
        except Exception as e:
            logger.error(f"Error during differential analysis: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self.construct_network()
        except Exception as e:
            logger.error(f"Error during network construction: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self.analyze_network()
        except Exception as e:
            logger.error(f"Error during network analysis: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self.validate_targets()
        except Exception as e:
            logger.error(f"Error during target validation: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self.generate_visualizations()
        except Exception as e:
            logger.error(f"Error during visualization generation: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        try:
            self._create_summary_report()
        except Exception as e:
            logger.error(f"Error creating summary report: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            success = False
            
        if success:
            logger.info("Pipeline completed successfully")
        else:
            logger.warning("Pipeline completed with some errors")
            
        return True

def main():
    parser = argparse.ArgumentParser(description='Drug Target Discovery Pipeline with Bioconductor Mapping')
    parser.add_argument('--matrix-file', type=str, default='GSE46602_series_matrix.txt.gz', help='Path to GEO Series Matrix file')
    parser.add_argument('--output-dir', type=str, help='Output directory for results')
    parser.add_argument('--p-threshold', type=float, default=0.05, help='P-value threshold for significance')
    parser.add_argument('--fc-threshold', type=float, default=1.0, help='Log2 fold change threshold for significance')
    args = parser.parse_args()
    
    pipeline = DrugTargetPipeline(
        matrix_file=args.matrix_file,
        output_dir=args.output_dir,
        p_threshold=args.p_threshold,
        fc_threshold=args.fc_threshold
    )
    
    success = pipeline.run_pipeline()
    if success:
        print(f"Pipeline completed successfully. Results saved to: {pipeline.output_dir}")
        return 0
    else:
        print("Pipeline failed. Check logs for details.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
